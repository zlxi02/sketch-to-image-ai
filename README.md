# âœ¨ Sketch-to-Image AI

A full-stack web application that transforms simple sketches into realistic images using Stable Diffusion with ControlNet, running entirely on your local machine.

![Demo](https://img.shields.io/badge/Status-Fully_Functional-success)
![Python](https://img.shields.io/badge/Python-3.11+-blue)
![React](https://img.shields.io/badge/React-18.2-61dafb)
![FastAPI](https://img.shields.io/badge/FastAPI-0.109-009688)

## ğŸ¯ Overview

Draw a simple sketch on the canvas, optionally add a text prompt, and watch as AI generates a photorealistic image based on your drawing. The entire pipeline runs locally on Apple Silicon (M1/M2/M3) Macs, with no external API calls or subscriptions required.

**Example:**
- **Input:** Simple sketch of stars, barn, and cow
- **Prompt:** "A farm at night with a barn and a cow and stars"
- **Output:** Photorealistic image of a red barn under starry sky with Milky Way, green grass, and a cow

## ğŸ—ï¸ Architecture

### Tech Stack

**Backend (Python):**
- **FastAPI** - Modern async web framework
- **PyTorch** - Deep learning framework with Apple Silicon (MPS) support
- **Diffusers** - Hugging Face's Stable Diffusion implementation
- **ControlNet** - Guides image generation using sketch structure
- **controlnet-aux** - HED edge detection preprocessing

**Frontend (React + Vite):**
- **React 18** - UI framework with hooks
- **Vite** - Lightning-fast dev server and build tool
- **HTML5 Canvas API** - Drawing interface
- **Fetch API** - HTTP requests to backend

### System Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Browser                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  React Frontend (localhost:5173)                     â”‚   â”‚
â”‚  â”‚  â€¢ Drawing Canvas (HTML5)                            â”‚   â”‚
â”‚  â”‚  â€¢ User draws sketch                                 â”‚   â”‚
â”‚  â”‚  â€¢ Optional text prompt input                        â”‚   â”‚
â”‚  â”‚  â€¢ Click "Generate"                                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”‚ HTTP POST /generate
                          â”‚ (FormData: sketch.png + prompt)
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FastAPI Backend (localhost:8000)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  1. Receive sketch image + prompt                    â”‚   â”‚
â”‚  â”‚  2. Preprocess: Resize to 512x512                    â”‚   â”‚
â”‚  â”‚  3. Apply HED edge detection                         â”‚   â”‚
â”‚  â”‚  4. Load Stable Diffusion + ControlNet models        â”‚   â”‚
â”‚  â”‚  5. Run inference (~10-30 seconds on M3)             â”‚   â”‚
â”‚  â”‚  6. Return base64-encoded generated image            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”‚ GPU Acceleration
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Apple Silicon (M1/M2/M3)                        â”‚
â”‚  â€¢ MPS (Metal Performance Shaders) backend                   â”‚
â”‚  â€¢ ~5GB model weights cached in ~/.cache/huggingface        â”‚
â”‚  â€¢ 16-core Neural Engine acceleration                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Implementation Strategy

### 1. Model Selection
- **Base Model:** `runwayml/stable-diffusion-v1-5` - Proven, well-optimized for various hardware
- **ControlNet:** `lllyasviel/sd-controlnet-hed` - HED (Holistically-Nested Edge Detection) variant
  - Excellent at preserving sketch structure while allowing creative freedom
  - Balances control vs. creativity better than canny or depth models

### 2. Backend Pipeline

**Image Preprocessing:**
```python
1. Input sketch (any size) â†’ Convert to RGB
2. Resize to 512x512 (SD v1.5 optimal resolution)
3. Apply HED edge detection to extract structural edges
4. Feed processed edges to ControlNet
```

**Inference Configuration:**
- **Inference Steps:** 20 (balance between speed and quality)
- **Guidance Scale:** 7.5 (moderate prompt adherence)
- **MPS Device:** Apple Silicon GPU acceleration
- **Float16:** Reduced memory usage without quality loss

**Performance Optimizations:**
- Model loaded once at startup (not per-request)
- Cached in memory for instant subsequent generations
- NumPy < 2.0 for PyTorch compatibility
- Async FastAPI endpoints for non-blocking I/O

### 3. Frontend Architecture

**Component Structure:**
```
App.jsx (Main orchestrator)
  â”œâ”€ DrawingCanvas.jsx (Canvas drawing logic)
  â”‚    â”œâ”€ Mouse event handlers
  â”‚    â”œâ”€ Canvas context management
  â”‚    â””â”€ Clear/Export functionality
  â”‚
  â””â”€ ImageDisplay.jsx (Result rendering)
       â”œâ”€ Loading state (spinner)
       â”œâ”€ Placeholder state
       â””â”€ Image display (base64 decode)
```

**State Management:**
- **React Hooks (useState)** for local component state
- **Refs (useRef)** for canvas element access
- **Props** for parent-child communication
- No external state library needed (simple app)

**Data Flow:**
```
User draws â†’ Canvas stores pixels â†’ Export to Blob â†’
FormData â†’ POST to backend â†’ Await response â†’
Base64 image â†’ Display in <img> tag
```

### 4. API Design

**Endpoint:** `POST /generate`

**Request:**
```
Content-Type: multipart/form-data
- file: sketch.png (binary)
- prompt: "descriptive text" (string, optional)
```

**Response:**
```json
{
  "image": "base64_encoded_png_string",
  "generation_time": 12.34,
  "message": "Image generated successfully"
}
```

**CORS Configuration:**
- Allows `localhost:5173` and `localhost:3000`
- Necessary for frontend-backend communication during development

## ğŸ“‹ Features

âœ… **Drawing Canvas**
- Freehand drawing with mouse
- Crosshair cursor for precision
- Clear canvas button
- Auto-exports to PNG

âœ… **AI Image Generation**
- Stable Diffusion v1.5 with ControlNet
- Optional text prompts for guidance
- Preserves sketch structure
- Photorealistic output

âœ… **User Experience**
- Real-time drawing feedback
- Loading spinner with progress indicator
- Error handling and user-friendly messages
- Responsive layout (works on different screen sizes)

âœ… **Performance**
- Self-hosted (no API costs)
- Apple Silicon GPU acceleration
- 10-30 second generation time
- Models cached locally

## ğŸ› ï¸ Setup Instructions

### Prerequisites

- **macOS** with Apple Silicon (M1/M2/M3)
- **Python 3.11+**
- **Node.js 18+** and npm
- **~10GB free disk space** (for model weights)

### Backend Setup

```bash
# Navigate to backend directory
cd backend

# Create and activate virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies (takes 3-5 minutes)
pip3 install -r requirements.txt

# Start the server
python3 app.py
```

Backend will run on `http://localhost:8000`

**First run:** Models will auto-download to `~/.cache/huggingface/` (~5GB)

### Frontend Setup

```bash
# Navigate to frontend directory
cd frontend

# Install dependencies
npm install

# Start development server
npm run dev
```

Frontend will run on `http://localhost:5173`

### Testing

Open browser to `http://localhost:5173`

1. Draw a simple sketch (house, face, animal, etc.)
2. Optionally add a text prompt (e.g., "realistic photo, detailed")
3. Click "ğŸš€ Generate Image"
4. Wait 10-30 seconds
5. View the AI-generated result!

## ğŸ“ Project Structure

```
sketch-to-image-ai/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                 # FastAPI server + endpoints
â”‚   â”œâ”€â”€ model_handler.py       # AI model loading & inference
â”‚   â”œâ”€â”€ requirements.txt       # Python dependencies
â”‚   â””â”€â”€ venv/                  # Virtual environment (gitignored)
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.jsx           # React entry point
â”‚   â”‚   â”œâ”€â”€ App.jsx            # Main app component
â”‚   â”‚   â”œâ”€â”€ App.css            # Styling
â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚       â”œâ”€â”€ DrawingCanvas.jsx       # Canvas drawing
â”‚   â”‚       â”œâ”€â”€ DrawingCanvas.css
â”‚   â”‚       â”œâ”€â”€ ImageDisplay.jsx        # Result display
â”‚   â”‚       â””â”€â”€ ImageDisplay.css
â”‚   â”œâ”€â”€ index.html             # HTML entry point
â”‚   â”œâ”€â”€ package.json           # npm dependencies
â”‚   â””â”€â”€ node_modules/          # npm packages (gitignored)
â”‚
â”œâ”€â”€ .gitignore                 # Git exclusions
â””â”€â”€ README.md                  # This file
```

## ğŸ¨ Usage Tips

**For Best Results:**
- Draw clear, simple outlines (not shaded/filled)
- Use bold strokes for main objects
- Add descriptive prompts ("realistic", "detailed", "sunset lighting")
- Experiment with different sketch styles

**Example Prompts:**
- "realistic photo, high quality, detailed"
- "oil painting, impressionist style"
- "digital art, concept art, artstation"
- "sunset lighting, golden hour"
- "black and white photography"

## ğŸ”§ Configuration

### Backend Configuration

Edit `backend/model_handler.py`:

```python
# Adjust generation quality
num_inference_steps=20    # Higher = better quality, slower (10-50)
guidance_scale=7.5        # Higher = closer to prompt (1-20)

# Change models
model_id = "runwayml/stable-diffusion-v1-5"
controlnet_model = "lllyasviel/sd-controlnet-hed"
```

### Frontend Configuration

Edit `frontend/src/App.jsx`:

```javascript
// Backend URL
const response = await fetch('http://localhost:8000/generate', {
  // Change if backend runs on different port
})
```

## ğŸ› Troubleshooting

**Issue:** "ModuleNotFoundError: No module named 'fastapi'"
- **Solution:** Activate virtual environment: `source venv/bin/activate`

**Issue:** "React is not defined"
- **Solution:** Ensure all component files import React: `import React from 'react'`

**Issue:** "CORS error" in browser console
- **Solution:** Ensure backend is running and CORS is configured for your frontend port

**Issue:** Generation takes too long (>60 seconds)
- **Solution:** Reduce `num_inference_steps` to 10-15 in `model_handler.py`

**Issue:** Out of memory errors
- **Solution:** Close other apps, or reduce image resolution to 256x256

## ğŸ“Š Performance Benchmarks

**Hardware:** M3 MacBook Pro (16GB RAM)

| Task | Time |
|------|------|
| Model loading (first time) | ~30 seconds |
| Model loading (cached) | Instant |
| Image generation | 10-30 seconds |
| Model download (first run) | 5-10 minutes |

**Memory Usage:**
- Backend idle: ~500MB
- Backend with models loaded: ~5GB
- Frontend: ~100MB

## ğŸ” Privacy & Security

âœ… **Completely Local**
- No data sent to external servers
- No API keys required
- No usage tracking or analytics
- Your sketches and images stay on your machine

âœ… **Open Source**
- All code visible and auditable
- Models from Hugging Face (open source)

## ğŸš§ Future Enhancements

Potential improvements:
- [ ] Multiple ControlNet models (canny, depth, pose)
- [ ] Adjustable brush size and color
- [ ] Image-to-image mode (upload reference)
- [ ] Gallery to save/view previous generations
- [ ] Export generated images as PNG/JPG
- [ ] Real-time generation progress bar
- [ ] Support for Stable Diffusion XL
- [ ] Mobile touch support

## ğŸ“š Technical Resources

**Papers:**
- [Stable Diffusion](https://arxiv.org/abs/2112.10752) (Rombach et al., 2022)
- [ControlNet](https://arxiv.org/abs/2302.05543) (Zhang et al., 2023)
- [HED Edge Detection](https://arxiv.org/abs/1504.06375) (Xie & Tu, 2015)

**Documentation:**
- [Hugging Face Diffusers](https://huggingface.co/docs/diffusers)
- [FastAPI](https://fastapi.tiangolo.com/)
- [React](https://react.dev/)

## ğŸ“ License

This project is open source and available under the MIT License.

## ğŸ™ Acknowledgments

- **Stable Diffusion** by Stability AI
- **ControlNet** by Lvmin Zhang
- **Hugging Face** for model hosting and diffusers library


---

**Built with â¤ï¸ for creative experimentation**

*Self-hosted AI on Apple Silicon - No cloud, no cost, no limits.*

